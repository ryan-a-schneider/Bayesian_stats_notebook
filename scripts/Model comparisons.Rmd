---
title: "Untitled"
output: html_document
date: "2023-04-25"
---

```{r setup, include=FALSE}

pacman::p_load(tidyverse, brms, easystats, loo)
options(mc.cores = 10)

load(here::here("models", "models_for_comparisons.RData"))

loo_compare_tidy=function(...){
  loo::loo_compare(..., criterion = c("loo")) |>
    as_tibble() |> # convert from a matrix to a tidy table
    rownames_to_column(var = "model") |> 
    mutate(across(c(2:9), round, 2)) # round columns 2-9 to 2 decimals
}
```

## Example from Statistical Rethinking

First add loo and WAIC info to models. Then compare them.

```{r}

b5.1 <- add_criterion(b5.1, criterion = c("waic", "loo"))
b5.2 <- add_criterion(b5.2, criterion = c("waic", "loo"))
b5.3 <- add_criterion(b5.3, criterion = c("waic", "loo"))


# 1. verify that LOOIC and WAIC are in agreement
performance::compare_performance(b5.1, b5.2, b5.3, metrics=c("LOOIC", "WAIC"))
  
```

As long as they are pretty close you're fine. If they aren't, something is wrong (no idea what).

Now compare WAIC (or LOOIC), and read the following:

<https://avehtari.github.io/modelselection/CV-FAQ.html#5_How_to_use_cross-validation_for_model_selection>

> ***HOW TO USE CROSS-VALIDATION FOR MODEL SELECTION?***
>
> First avoid model selection by using the model which includes all predictors and includes all uncertain things. Then optimal thing is to integrate over all the uncertainties. When including many components to a model, it is useful to think more carefully about the prior. For example, if there are many predictors, it is useful to use priors that a) state that only some of the effects are big, or b) many effects are big and correlating (it is not possible to have a large number of big independent effects Tosh *et al.* ([2021](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Tosh+etal:2021:piranha))).
>
> If there is explicit utility or loss for observing future predictor values (e.g. medical tests) use decision theory.
>
> If there is implicit cost for bigger models (e.g. bigger model more difficult to explain or costs of feature measurements are unknown), choose a smaller model which similar predictive performance as the biggest model. If there are only a small number of models, overfitting due to selection process is small. If there are a large number of models, as for example often in variable selection, then the overfitting due to the selection process can be a problem (Piironen and Vehtari, [2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Piironen+Vehtari:2017a)) and more elaborate approaches, such as projection predictive variable selection is recommended.
>
> If there is application specific utility or loss function, use that to assess practically relevant difference in predictive performance of two models.
>
> **If there is no application specific utility or loss function, use log score, ie elpd**.
>
> -   If elpd difference (elpd_diff in `loo` package) is less than 4, the difference is small (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty))).
>
> -   If elpd difference (elpd_diff in loo package) is larger than 4, then compare that difference to standard error of elpd_diff (provided e.g. by `loo` package) (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty)). See also Section [How to interpret in Standard error (SE) of elpd difference (elpd_diff)?](https://avehtari.github.io/modelselection/CV-FAQ.html#se_diff).
>
> If there is a large number of models compared, there is possibility of overfitting in model selection. See video [Model assessment, comparison and selection at Master class in Bayesian statistics, CIRM, Marseille](https://www.youtube.com/watch?v=Re-2yVd0Mqk).
>
> Vehtari and Ojanen ([2012](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+Ojanen:2012)) write: "The model selection induced bias can be taken into account by the double/nested/2-deep cross-validation (e.g. Stone, 1974; Jonathan, Krzanowski and McCarthy, 2000) or making an additional bias correction (Tibshirani and Tibshirani, 2009)."
>
> Piironen and Vehtari ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Piironen+Vehtari:2017a)) write: "Although LOO-CV and WAIC can be used to obtain a nearly unbiased estimate of the predictive ability of a given model, both of these estimates contain a stochastic error term whose variance can be substantial when the dataset is not very large. This variance in the estimate may lead to over-fitting in the selection process causing nonoptimal model selection and inducing bias in the performance estimate for the selected model (e.g., Ambroise and McLachlan 2002; Reunanen 2003; Cawley and Talbot 2010). The overfitting in the selection may be negligible if only a few models are being compared but, as we will demonstrate, may become a problem for a larger number of candidate models, such as in variable selection."
>
> Nested CV helps to estimate the overfitting due to the selection but doesn't remove that. The overfitting is more severe depending on how many degrees of freedom there are in the selection. For example, in predictor selection we can think that we as many indicator variables as there are predictors and then there are combinatorial explosion in possible parameter combinations and overfitting can be severe (as demonstrated by Piironen and Vehtari ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Piironen+Vehtari:2017a))).

```{r}

loo_compare_tidy(b5.1, b5.2, b5.3)
```

*In the tables above...*

### ELPD

`elpd_loo` represents the expected out-of-sample log-predictive density (ELPD) of each model, which is calculated using PSIS-LOO-CV (an approximation of leave-one-out cross-validation). **ELPD quantifies the average log-likelihood of new data points given the model's posterior distribution. In essence, it quantifies how well the model is able to predict new data.** It takes into account both the model's ability to fit the observed data (log-likelihood) and its complexity (penalization for model complexity).

-   The larger the ELPD, the better the model is at predicting new, unobserved data. (Note that these values only have meaning *relative to other ELPD's, however; you can't interpret them on their own.*)

`elpd_diff` shows the difference between models' ELPD; it's a direct comparison of their estimated out-of-sample prediction capabilities. **This is the column you're interested in.** `elpd_se` is the standard error associated with this estimate. "[As quick rule:](https://avehtari.github.io/modelselection/CV-FAQ.html#12_What_is_the_interpretation_of_ELPD__elpd_loo__elpd_diff)"

> If elpd difference (`elpd_diff` in `loo` package) is less than 4, the difference is small [and the models have "very similar predictive performance"] (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty)).
>
> If elpd difference (`elpd_diff` in loo package) is larger than 4, then compare that difference to standard error of `elpd_diff` (provided e.g. by `loo` package) (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty)). The value for deciding what is small or large can be based on connection to Pseudo-BMA+-weights (Yao *et al.*, [2018](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Yao+etal:2018)) (Vehtari, from link above)
>
> ... When the difference (`elpd_diff`) is larger than 4, the number of observations is larger than 100, and the model is not badly misspecified, then normal approximation and SE are quite reliable description of the uncertainty in the difference.

### Information Criterion

`looic` and `elpd_waic` are Info Criterion based the concept of cross-validation. [***They estimate ELPD***](https://avehtari.github.io/modelselection/CV-FAQ.html#21_How_are_LOO_and_WAIC_related)***.*** The model is trained with either point i left out (LOOIC) or several points left out (WAIC); the log-likelihood of the left-out data point is recorded; and the process repeated.

-   They should produce very similar results and be very close in their estimates, even though they are computed differently and handle model complexity differently. *Lower values indicate better performance.*

-   However, note the following from [Vehtari's blog](Vehtari,%20Gelman%20and%20Gabry%20(2017)%20show%20that%20PSIS-LOO%20has%20usually%20smaller%20error%20in%20estimating%20ELPD%20than%20WAIC.%20The%20exception%20is%20the%20case%20when%20p_loo%20≪N,%20as%20then%20WAIC%20tends%20to%20have%20slightly%20smaller%20error,%20but%20in%20that%20case%20both%20PSIS-LOO%20and%20WAIC%20have%20very%20small%20error%20and%20it%20doesn’t%20matter%20which%20computational%20approximation%20is%20used.%20On%20the%20other%20hand,%20for%20flexible%20models%20WAIC%20fails%20more%20easily,%20has%20significant%20bias%20and%20is%20less%20easy%20to%20diagnose%20for%20failures.%20WAIC%20has%20been%20included%20in%20loo%20package%20only%20for%20comparison%20purposes%20and%20to%20make%20it%20easy%20to%20replicate%20the%20results%20in%20Vehtari,%20Gelman%20and%20Gabry%20(2017).), that **suggests you should probably prefer LOOIC to WAIC: "**Vehtari, Gelman and Gabry ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017)) show that PSIS-LOO has usually smaller error in estimating ELPD than WAIC. The exception is the case when p_loo ≪N, as then WAIC tends to have slightly smaller error, but in that case both PSIS-LOO and WAIC have very small error and it doesn't matter which computational approximation is used. On the other hand, for flexible models WAIC fails more easily, has significant bias and is less easy to diagnose for failures. WAIC has been included in `loo` package only for comparison purposes and to make it easy to replicate the results in Vehtari, Gelman and Gabry ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017))

`p_loo` is the effective number of parameters in the model. It is a penalty term applied to each model that represents model complexity. Per [Vehtari](https://avehtari.github.io/modelselection/CV-FAQ.html#19_What_is_the_interpretation_of_p_loo):

> It is not needed for `elpd_loo`, but has diagnostic value. It describes how much more difficult it is to predict future data than the observed data. Asymptotically under certain regularity conditions, `p_loo` can be interpreted as the effective number of parameters.
>
> `p_loo` \>N or `p_loo` \>p indicates that the model has very weak predictive capability. This can happen even in case of well specified model (as demonstrated in Figure 1 in Vehtari, Gelman and Gabry ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017))), but may also indicate a severe model misspecification.
>
> See more in "Interpreting p_loo when Pareto-k\^ is large" in [LOO Glossary](https://mc-stan.org/loo/reference/loo-glossary.html).

Comparing 5.3 to 5.1:

$$
0.35_{se}*4=1.4
$$

$$
1.4>-0.87_{diff}
$$

Since the difference between the models is not bigger than 4 SE's, there is not a meaningful difference in predictive accuracy between 5.1 and 5.3.

Given that 5.1 is the simpler model, that's the one we'd pick.

ALTERNATIVELY:

Check to see that the difference between the models' ELPD is at least 2 standard error's below zero.

$$
-0.87_{diff}+(2*0.35_{se})=0.7
$$

$$
-0.87_{diff}+0.7=-0.17
$$

Since $-0.17<0$, there is a meaningful difference between the models.

Lets investigate further.

## Diagnostics: Checking Pareto-K

[https://avehtari.github.io/modelselection/CV-FAQ.html#17_What_to_do_if_I\_have_many_high_Pareto\_(hat{k})%E2%80%99s](https://avehtari.github.io/modelselection/CV-FAQ.html#17_What_to_do_if_I_have_many_high_Pareto_(hat%7Bk%7D)%E2%80%99s)

> The Pareto-k\^ diagnostic estimates how far an individual leave-one-out distribution is from the full distribution. If leaving out an observation changes the posterior too much then importance sampling is not able to give reliable estimate.
>
> -   If k\^\<0.5, then the corresponding component of `elpd_loo` is estimated with high accuracy.
>
> -   If 0.5\<k\^\<0.7 the accuracy is lower, but still OK.
>
> -   If k\^\>0.7, then importance sampling is not able to provide useful estimate for that component/observation.
>
> Pareto-k\^ is also useful as a measure of influence of an observation. Highly influential observations have high k\^ values. Very high k\^ values often indicate model misspecification, outliers or mistakes in data processing. [In cases where k\>0.7, check `p_loo`...
>
> -   If p_loo ≪p (the total number of parameters in the model), then the model is likely to be misspecified. Posterior predictive checks (PPCs) are then likely to also detect the problem. Try using an overdispersed model, or add more structural information (nonlinearity, mixture model, etc.).
>
> -   If p_loo \<p and the number of parameters p is relatively large compared to the number of observations (e.g., p\>N/5), it is likely that the model is so flexible or the population prior so weak that it's difficult to predict the left out observation (even for the true model). This happens, for example, in the simulated 8 schools (Vehtari, Gelman and Gabry, [2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017)), random effect models with a few observations per random effect, and Gaussian processes and spatial models with short correlation lengths.
>
> -   If p_loo \>p, then the model is likely to be badly misspecified. If the number of parameters p≪N, then PPCs are also likely to detect the problem. See for example the [Roaches case study](https://avehtari.github.io/modelselection/roaches.html). If p is relatively large compared to the number of observations, say p\>N/5 (more accurately we should count number of observations influencing each parameter as in hierarchical models some groups may have few observations and other groups many), it is possible that PPCs won't detect the problem.

```{r}
loo(b5.3)
```

Model 5.3 has one estimate in the "ok" range. So there's at least one piece of data that didn't quite measure up to "good" status. Which piece was that? Let's filter by using the minimum upper bound specified for "good."

```{r}
loo(b5.3) |> pareto_k_ids(threshold = 0.5)
```

Looks like it was the value in the 13th row of the data. You can filter your data to find out which point that is.

You can also check the specific pareto-k value for that individual data point.

```{r}
b5.3$criteria$loo$diagnostics$pareto_k[13]
```

In this case, that value isn't great, but not bad enough (i.e., at or worse than .7) to return a warning message.

## Other stuff

### Qualitative Posterior Predictive Checks

```{r}

see::plots(bayesplot::pp_check(b5.1),
           bayesplot::pp_check(b5.2),
           bayesplot::pp_check(b5.3)
           )
```

### How does a Bayes factor compare to LOO-CV? (from Vehtari's blog)

<https://avehtari.github.io/modelselection/CV-FAQ.html#24_What_is_the_relationship_between_LOO-CV_and_Bayes_factor>

> -   LOO-CV estimates the predictive performance given N−1 observations. Bayes factor can be presented as ratio of predictive performance estimates given 0 observations. Alternatively Bayes factor can be interpreted as choosing the maximum a posterior model.
>
> -   Bayes factor can be sensible when models are well specified and there is lot of data compared to the number of parameters, so that maximum a posteriori estimate is fine and the result is not sensitive to priors
>
> -   If there is not a lot of data compared to the number of parameters, Bayes factor can be much more sensitive to prior choice than LOO-CV
>
> -   If the models are not very close to the true model, Bayes factor can be more unstable than cross-validation (Yao *et al.*, [2018](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Yao+etal:2018); Oelrich *et al.*, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Oelrich+etal:2020:overconfident)).
>
> -   Computation of Bayes factor is more challenging. For example, if computed from MCMC sample, usually several orders of magnitude bigger sample sizes are needed for Bayes factor than for LOO-CV
>
> -   If the models are well specified, regular, and there is a lot of data compared to the number of parameters (n≫p), then Bayes factor may have smaller variance than LOO-CV. If the models are nested, instead of Bayes factor, it is also possible to look directly at the posterior of the interesting parameters (see also 2b in [Using cross-validation for many models](https://avehtari.github.io/modelselection/CV-FAQ.html#manymodels))
