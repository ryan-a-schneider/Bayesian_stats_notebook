---
title: "Untitled"
output: html_document
date: "2023-06-20"
---

```{r}

```

## Logistic Regression Example

### Model Overview and Interpretation of Parameters

The formula for logistic regression is:

$$
logit(\mu)=\beta_0 + \beta_1X_1 + \beta_2X_2
$$

When $X_1$ goes up by one unit (on whatever scale it's on), then the $logit(\mu)$ increases by amount $\beta_1$ ; When $X_2$ increases by 1 unit, $logit(\mu)$ increases by $\beta_2$.

See [this link](https://yury-zablotski.netlify.app/post/how-logistic-regression-works/) and [this link](https://stats.oarc.ucla.edu/r/dae/logit-regression/) for help.

Logistic regression uses a linking function to turn a dichotomous variable into a linear one.

-   **Odds** are the proportion of an event occuring to the proportion of it not-happening. If 100 people were in one of our study conditions and 60 of them pled guilty, then the odds of pleading guilty are 60:40, or $\frac{6}{4}$ . When divided out, the odds of pleading guilty are 1.5x

-   **Log-odds** are the logarithm of the odds, which transforms their scale to make it linear and symmetrical around 0. Negative log-odds indicate a lower chance of success, while positive log-odds indicate a higher chance of success. **This is the default output for a coefficient in logistic regression.**

-   An **Odds Ratio** is a ratio of two odds. This tells you the change in odds moving from one group to another, or the change in odds as the IV increases one point. ***These are not symmetrical around 0*** like log-odds are. You can get an Odds Ratio by exponentiating the log odds.

-   **Probability** is the ratio of an event happening, compared to all possible outcomes.

```{r}
# make a model
m <- glm(formula = am ~ mpg, data = mtcars, family = binomial())

# bayesian
fit_logistic = brm(data = mtcars,
                   family = binomial,
                   formula = am | trials(1) ~ mpg,
                   prior = c(prior(normal(0, 2), class = Intercept),
                             prior(normal(0, 2), class = b)),
                   chains = 2, iter = 2000, warmup = 750, seed = 3)

# grab results
parameters(m) |> mutate(Odds_Ratio=exp(2)) |> relocate(Odds_Ratio, .before = 2)

fixedef(fit_logistic)
```

-   The coefficient of 0.31 for mpg is interpreted as the **expected change in log odds for a one-unit increase in the IV.**

-   If you turn this into an odds ratio, you get 1.19. *For every 1-point increase in the predictor (MPG), the odds of a car being a manual transmission increase by a factor of 1.19 times*

-   (if this was a categorical variable, it would say that the odds are x% higher for group A vs. group B)

#### Bonus: Translating Odds Ratios into (predicted) probability

You can also compute predicted probabilities to understand the model in a different way. This can be done for both continuous and categorical predictors. Probability can be extracted from log-odds.

```{r}

# add predictions to the original data
mtcars <- mtcars |> 
  mutate(log_odds = predict(m), # only necessary to create the ggplot below
         preds_prob = predict(m, type = "response"))

# visualize log-odds and probabilities to see their connection
ggplot(mtcars)+
  geom_point(aes(mpg, am), color = "orange")+
  geom_point(aes(mpg, preds_prob))+
  geom_line(aes(mpg, preds_prob))+
  geom_point(aes(mpg, log_odds), color = "blue")+
  geom_line(aes(mpg, log_odds), color = "blue")+
  geom_vline(xintercept = 21.5, linetype = "dotted")+
  geom_hline(yintercept = 0.5, linetype = "dotted")+
  theme_minimal()
```

-   The blue line shows the log-odds of each car being a manual transmission, based on its MPG.

-   The black S curve shows the probabilities of each car being a manual, based on its MPG

Taking one car from this as an example...

```{r}
mtcars |> slice(19) |> select(mpg, log_odds, am, preds_prob)
```

A car with an MPG rating of 30 has a 93% probability of being a manual transmission
