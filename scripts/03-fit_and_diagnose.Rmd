---
title: "Fit and Diagnose"
output: html_document
date: "2023-06-20"
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, brms, easystats)
options(mc.cores = parallel::detectCores(logical = FALSE)-1)
```

# Part 2: Fitting and Diagnosing a Bayesian Model

## Preliminary info on brms

Stuff to know about...

`iter`, `chains`, and `warmup`

-   `iter` is the number of steps taken in each chain. Kurz in chapter 4 notes: "you usually only need to set `iter= 2000, warmup=1000` when using moderately better priors [than these terrible uniform ones]." You usually don't need more than a few thousand steps per chain, if you have a relatively simple model and you're only trying to get estimates of the parameters. **However,** if you're going to use Bayes factors, then you need at least 40,000 posterior samples.

-   `warmup` is how many of the steps specified in `iter` will be used for a "warm-up" run and discarded.

-   You don't need more than 3-4 chains for most models. Running multiple chains helps ensure model stability and accuracy (in terms of the Monte Carlo estimates); but there isn't much benefit to running more than 4 or 5 chains.

-   Monte Carlo chains run serially, and thus can only be run on a single CPU core, so telling BRMS to use all 8 cores (if you have an 8-core CPU) will not grant any speed up in model compilation time...

-   If you're looking to speed up model compile times, you need to run special code that samples in parallel. Or, use the GPU instead of the CPU to compile the code, via the cmdstanr package. Both are very complicated and not worth it for our simple models. *This is only worth it for stuff that takes hours or days to compile.*

-   Sometimes you'll hear about "burn in" in discussions on MCMC, and what an appropriate rate is for the sampler. That does not apply to brms or rstanarm. Also, it is not the same thing as warm-up steps. Ignore it for now.

## How to Fit a Model with brms

-   Note that `coef= x` does not have to be specified. It's only necessary if you do not want to apply the same prior to every effect in the model (which would be done by simply leaving it `class = b`)
-   If using Bayes factors, multiply \`iter\` x10 so you have 50-60k steps needed instead of 5-6k. Bayes factors need a very large ESS per parameter
-   Also if using Bayes factors, make sure each model has `diagnostic_file = file.path(tempdir(), "df.csv")` (if using rstanarm) or `save_pars = save_pars(all = TRUE)` (if using brms), and remember to change the name of each `df` file when you copy-paste that line from one model to the next!

```{r}
##### Basic linear model example ####
brms_fit <-brm(moptim ~ sex,
               family = gaussian(link = "identity"),
               data = survey,                
               prior = c(
                 prior(normal(3, 2), class = Intercept), # expected mean and dev. of DV, on the scale of the DV's unit
                 prior(normal(0, 3), class= sigma), 
                 prior(normal(0, 3), class = b, coef= sexMALES),
               iter = 28000, warmup = 27000, chains = 3, cores = 3, seed = 4))


##### Basic logistic models #######

fit <-
  brm(data = my_data, 
      family = bernoulli, # only works with NON-aggregated data
      y ~ 1 + x1 + x2,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)))

fit <-
  brm(data = my_data, 
      family = binomial, # works with both aggregated and non-agg data
      y | trials(1) ~ 1 + x1 + x2,
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 2), class = b)))

```

***Some quick notes on the logistic fits:***

-   For the logistic models, the `family=binomial` version will only work with a DV that is stored as a `double` or `integer` type variable; it does not work with dichotomous `factor` variables. If you have a factor, use the Bernoulli version.

-   If you have aggregated data, you must use the latter version that models data on the `binomial` distribution (which makes intuitive sense, given that the Bernoulli distribution is for modeling only a single binomial event)

### How to "update" An Existing Model Fit

Rather than use the above code to "fit" several models that have slight differences between them, you can sometimes fit one model, and then update it with the following code. This...

1.  Saves lines of code
2.  By proxy, makes things neater and reduces copy-paste errors
3.  And may allow you to get around having to re-compile the code for the model before sampling, which dramatically reduces the computation time.

```{r}

# create a new model by updating an initial one, by adding the intercept parameter back in
m2 <-update(brms_index, newdata = practice_data,
                           formula. = ~ .+1,
                           prior = c(prior(normal(3, 2), class = Intercept)))

# update again to create a third model with more predictors
m3 <-update(brms_dummy_single, newdata = practice_data,
                          formula. = ~ .-poc + poc_5 + poc_8)
```

## Check Your Variables

-   If you plan to put an interaction term into your model, or anything that is squared, cubed, etc., this dramatically increases collinearity and thus may make your model violate assumptions. So **mean-center any continuous variables you are putting in the model's interaction term**

-   Consider standardizing variables, especially for variables with drastically different scales (e.g., IQ vs. a 7-point Likert scale). Makes it easier to think about change in one causing a change on the other if they're both in Standard Deviation units.

    -   Can be accomplished with either `parameters::standardized_parameters(x, method = "refit")` or `parameters::model_parameters(model, standardize = "x", method = "refit")`

## Creating a Test Model (i.e., Prior Predictive Checking)

The first thing you need to do is pick a sensible prior. See [this page by the Stan development team](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) for suggestions on default, weakly-informative priors.

The code for this model will be identical to your final model, except for one thing: add `sample_prior="only"`

```{r}
b5.1 <- 
  brm(data = d, 
      family = gaussian,
      d ~ 1 + a,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      sample_prior = "only",
      file = "../fits/b05.01")
```

Then extract draws from the prior using the command noted above and plot

```{r}

#### Kurz's method in his translation of Statistical Rethinking ####

some_draws=brms::prior_draws(b5.1)

some_draws %>% 
  slice_sample(n = 50) %>% 
  rownames_to_column("draw") %>% 
  expand(nesting(draw, Intercept, b),
         a = c(-2, 2)) %>% 
  mutate(d = Intercept + b * a) %>% 
  
  ggplot(aes(x = a, y = d)) +
  geom_line(aes(group = draw),
            color = "firebrick", alpha = .4) +
  labs(x = "Median age marriage (std)",
       y = "Divorce rate (std)") +
  coord_cartesian(ylim = c(-2, 2)) +
  theme_bw() +
  theme(panel.grid = element_blank())


#### Default plots to do this super fast and easy ####
plot(b5.1)
```

## Diagnostic Checks (for the Final Model)

### A. Monte Carlo Diagnostics

After you've built your final model, check to see the chains converged.

Estimated Sample Size (ESS) and Rhat are two MCMC diagnostic indicators for MCMC/HMC models invented and used by Vehtari, Gelman, and the like. Good indicators to check, and easy to do so with the easystats suite.

Rhat should be \<1.1, and ESS should be \> 1000.

```{r}
 # extract info from model
diag=bayestestR::diagnostic_posterior(b5.3)

# check Rhat
diag$Rhat |> effectsize::interpret_rhat()

# check ESS
diag$ESS |> effectsize::interpret_ess()
```

### B. POSTERIOR Predictive Check

Posterior predictive checks Checks compare the observed data to replicated data from your model. If the model is a good fit and lines up with the replicated data, you should be able to generalize the model to new data sets. [Fife (2020)](https://journals.sagepub.com/doi/pdf/10.1177/1745691620917333?casa_token=Topmd-Kl6MEAAAAA:_wlfSAHi4G0EshY0ISz4DSQyMZkyxzzFZYxZzQfitVqSs4z2ILAyjlWEYSC8ZIG9qeRO-tXbBQvuHw) recommends checking a visual plot of the model's predictions as one of the first things you should do in data analysis (step 4, p. 1602).

This is an important step for every model, even if you're only build the one. ***If you're building multiple models and trying to compare competing theories, this step should be incorporated into model selection....***

> "Model building and model choice in the frequentest domain are based primarily on choosing the model that best fits the data. In the Bayesian domain, the choice among a set of competing models is based on which model provides the best posterior predictions. That is, the choice among a set of competing models should be based on which model will best predict what actually happens." p. 98

> In order to check your model and compare models, you need to do Posterior Predictive Checks. "The general idea behind posterior predictive checking is that there should be little, if any, discrepancy between data generated by the model and the actual data itself. In essence, posterior predictive checking is a method for assessing the specification quality of the model. Any deviation between the data generated from the model and the actual data implies model misspecification." p.99

There are many, many ways of doing this. Here's one method, done by Kurz, in his translation of *Statistical Rethinking* that plots a regression line on the data. First extract the values predicted by the model with `fitted()`, as noted above; then combine this with the data and pipe to ggplot

Alternatively, you can plot and compare the predicted posterior densities via any of the following:

```{r}
pp_check(b5.3)
```

Note that the above is mainly for *linear* regression. For logistic models, see [advice from Vehtari](https://discourse.mc-stan.org/t/posterior-predictive-check-for-binomial-regression/7203/4).

### C. Model-Specific Assumption Checks

You can check basically all major model requirements/assumptions for a linear model at once with the following:

```{r}
model_checks=performance::check_model(b5.3)
model_checks
```

Descriptions of all major assumptions (for linear regression) follow forthwith. (See *R for Psychology,* section 15.9, page 488 in the pdf).

Note that...

> If the visual inspection of the residuals signals problems, one may have to iterate through steps 2, 3, and 4 (i.e., of Fife, 2020) until the assumptions have been met, each time making a modification to the model (e.g., transforming the DV, removing outliers, using weighted least squares, or using generalized linear models.

#### Consistent/Constant Error Variance

This is called homogeneity of variance (or homo/heteroscadesticity in the context of ANOVA).

If the variances in the groups are different, their distributions are different, which makes them incomparable. If you violate this assumption you need to do something to fix it before moving forward. Usually you'd use Lavene's Test

> Strictly speaking, the regression model assumes that each residual is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation Ïƒ that is the same for every single residual. In practice, it's impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of YË† , and (if we're being especially paranoid) all values of every predictor X in the model...
>
> The main thing to worry about, if homogeneity of variance is violated, is that the standard error estimates associated with the regression coefficients are no longer entirely reliable, and so your t tests for the coefficients aren\'t quite right either... (R for psychology)

```{r}
plot(model_1, 3) # horizontal line with equally spread points is a good indication

performance::check_model(b5.3, check = "homogeneity")
```

#### Linearity of Residuals

Check a graph of the residuals of the model vs. its fitted predictions.

> A pretty fundamental assumption of the linear regression model is that relationship between X and Y actually be linear! Regardless of whether it's a simple regression or a multiple regression, we assume that the relationships involved are linear. (R for Psychology, p. 474)

For help: <https://cran.r-project.org/web/packages/tidybayes/vignettes/tidybayes-residuals.html>

```{r}
# base R
plot(model_1, 1) # horizontal line with equally spread points is a good indication


# easystats version
check_model(b5.3, check = "linearity")

# manual version with tidybayes and ggplot
stuff=tibble(point_preds=fitted(b5.3)[, 1], # extract and save model fitted values
             point_errs=residuals(b5.3)[, 1]) # extract and save model residuals

stuff |> 
  ggplot(aes(x= point_preds, y=point_errs)) +
  geom_point() +
  geom_smooth()
```

#### Normality of Residuals

In multiple regression, the assumption of normality applies only to the residuals, *not* the independent variables.

> Like half the models in statistics, standard linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It's actually okay if the predictors X and the outcome Y are non-normal, so long as the residuals are normal. See Section (R for psychology)

You should prefer visual diagnostics to significance checks here:

> It is common...to perform statistical tests of normality or homoskedasticity or independence. I would advise against it. Like other tests of significance, these tests of assumptions are sensitive to sample size...these tests tell us whether our distributions depart from what is expected. They do not tell us whether they are different enough to muck up our analysis. The latter is better done through visual interpretation of results (as well as through a sensitivity analysis [(Fife, 2020)](https://journals.sagepub.com/doi/pdf/10.1177/1745691620917333?casa_token=eX1tacsFYX0AAAAA:KBx-m-9gzt46-daDdypUH4fluLwkhxgZyey51Mo6PwXLppXgnJ8L0-_eurx5kvXXE7aWVzUU3dXUbQ)

Visual graphs of residuals show if the variables are following a path that would be expected, if your data was normally distributed. Only looking for visual evidence of "extreme" problems. You can use two different types of graphs to check this:

-   Histogram/density function of the residuals

-   A PP-plot or QQ-plot. PP-plots show the expected cumulative probability for a z-distribution by the observed cumulative probability; QQ plots show the same thing but with quantiles.

```{r}
#### Visual checks ####
  
plot(model_1, 2) # Q-Q plot. Residuals should follow the dashed line, which would indicate even-prediction error across the model. If the shape is anything other than a straight line it indicates possible miss-specification or non-linear relationships between variables


# QQ-plot for brms
diamonds |> 
  tidybayes::add_residual_draws(brms_fit) |> 
  tidybayes::median_qi() |> 
  ggplot(aes(sample = .residual)) + 
    geom_qq() +
    geom_qq_line()

hist(model_1$residuals)
```

The *Kolmogorov-Smirnov Test* or the *Shapiro-Walk test* both work as significance tests of this assumption. Both basically compare data with a normal distribution that has the same M and SD. A p \< .05 means that distribution of variable is significantly different from normal. This will likely be significant with large sample, so it should be combined with a visual check!!!

Note that this test almost always yields significant results for the distribution of residulas; visual inspection (QQ-plots) are preferable!!!!!

```{r}
#### Statistical test checks ####
check_normality(model_1) # Shapiro-Wilks test. 
broom::tidy(shapiro.test(model_1$residuals))
```

#### Other assumptions

##### Multicolinearity (Predictors are uncorrelated)

If your variables aren't really adding unique variance and there's tons of overlap, that will cause prediction issues. It is important to check multicolinearity to verify that your predictors are adding unique variance prediction to the model. Extreme collinearity can occur when what appear to be separate variables are actually measure the same thing.

> This is really just a "catch all" assumption, to the effect that "there's nothing else funny going on in the residuals". If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up. (R for psychology)

You have three different options to do this...

1.  Calculate ð‘…\^2 between each variable and all the rest. High values (e.g., \> .90) suggest extreme multivariate collinearity

2.  Check **Tolerance**: calculated as $1-R^2$. Values less than .10 may indicate extreme multivariate collinearity

3.  Check the **Variance inflation factor (VIF)**: calculated as 1/(1 âˆ’ ð‘…\^2). Values greater than 10.0 may suggest redundancy in observed variables

Note that VIF and Tolerance are almost the same; VIF is just a transformation of the other. Just pick one of the three above and report.

```{r}
performance::check_collinearity(model_2)
```

Example write up:

> Tests to check the assumption of collinearity indicated that correlation between the predictors was not an issue (Sex, *VIF*= 1.00; Child, *VIF*=1.00; Mean Negative Affect, *VIF*=1.01).

##### Autocorrelation

Including interaction terms in regression models tends to make the autocorrelation shoot through the roof (because these terms are a bit redundant). So you should check this.

Anything between 1 and 2 is ok. Usually anything less than 5 is good, but some people go up to 10.

```{r}

# run checks
car::durbinWatsonTest(model_3)
performance::check_autocorrelation(model_3) # easystats version
```
