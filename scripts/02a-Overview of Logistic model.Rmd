---
title: "Untitled"
output: html_document
date: "2023-06-20"
---

```{r}
pacman::p_load(tidyverse, brms, easystats)
```

# Logistic Regression

## Model Overview and Interpretation of Parameters

The formula for logistic regression is:

$$
logit(\mu)=\beta_0 + \beta_1X_1 + \beta_2X_2
$$

When $X_1$ goes up by one unit (on whatever scale it's on), then the $logit(\mu)$ increases by amount $\beta_1$ ; When $X_2$ increases by 1 unit, $logit(\mu)$ increases by $\beta_2$.

The $\beta$ coefficient in logistic regression represents change in log-odds. **Log-odds** are the logarithm of the odds, which transforms their scale to make it linear and symmetrical around 0. Negative log-odds indicate a lower chance of success, while positive log-odds indicate a higher chance of success. *This is a partial correlation; it tells you the direction and strength of the relationship.*

Note that this is very different from **probability**, which is the probability of an event happening, relative to all possible outcomes (i.e., rather than "not-happening").

For example:

-   If 100 people were in one of our study conditions and 60 of them pled guilty, then the odds of pleading guilty are 60:40, or $\frac{6}{4}$ . When divided out, the odds of pleading guilty are 1.5x. In contrast, the probability of pleading guilty would be 60%, or $\frac{60}{100}$

Exponentiating the log-odds gets you the **Odds Ratio.** This is a ratio of two odds, and represents *the change in the odds as you move from one situation or group to another. **These are not symmetrical around 0*** like log-odds are.

***Example***

-   Imagine that Hiliary Clinton's ***probability*** of winning the 2020 election, the outset of the race, was 66%. Naturally, this means she has a 33% probability of losing.

-   This means her ***odds of winning*** are twice as great as losing. This is expressed as: $\frac{2}{1}$

-   Then as the election looms, the models are re-run, and her estimated probability of winning changes to 80%. This would change the odds of winning to $\frac{4}{2}$.

-   This does *not* mean that she now has double the chances of winning! The actual probability of winning increased by 20%, not by a factor of two times. But her overall odds of winning are now double. This is what the Odds Ratio describes:

$$
\frac{\frac{4}{2}}{\frac{2}{1}}= 2
$$

If you really wanted to, you can extract probabilities from the logit function to make easier interpretations or predictions:

$$
probability=\frac{LogOdds}{1+LogOdds}
$$

See [this link](https://yury-zablotski.netlify.app/post/how-logistic-regression-works/) and [this link](https://stats.oarc.ucla.edu/r/dae/logit-regression/) for help.

### Confusing! Why!?

Why not just convert everything to probabilities and not deal with log-odds? Because while probability is using for understanding the probability of a specific event occurring (or not), odds are much more helpful for understanding the relative probabilities of several possible things occurring...

When you want to *compare the relative probability of one thing to another*, odds are much more useful. Odds and log-odds can describe how probabilities change; and it is this increase or decrease in the odds of an event occurring, and the strength of this change in probability, that is of interest (most of the time) in our models.

### Words of Caution

An odds ratio is the effect size for logistic regression. It describes the magnitude of the change in probability...

...but like all other effect sizes, it is descriptive, not an inferential statistic. You must, (1) use and interpret the data to show differences between conditions, and (2) not rely, heuristically, on a specific size for an odds ratio as an indicator of change.

## Example

```{r}
# make a model
m <- glm(formula = am ~ mpg, data = mtcars, family = "binomial")

# bayesian
fit_logistic = brm(data = mtcars,
                   family = bernoulli,
                   formula = am ~ 1 + mpg,
                   prior = c(prior(student_t(5, 0, 4), class = Intercept),
                             prior(student_t(5, 0, 1), class = b)),
                   chains = 4, iter = 2000, warmup = 750, seed = 3)

# grab results
parameters(m, exponentiate = FALSE)

parameters(fit_logistic, exponentiate = FALSE, ci=.89, ci_method = "HDI")
```

In the mtcars data set, 13 out of 32 cars are manual transmissions...

-   The probability that a *randomly selected car* would be a manual is therefore $13/32=0.40$ , or 40% (i.e., the proportion out of the total sample space).

-   The *odds* of a car being a manual are $0.4/0.6=0.66$, or more easily interpreted in fraction form as $\frac{40}{60}=\frac{4}6=\frac{2}{3}$ . Thus, the odds of a car being a manual transmission are 2:3; out of every 5 cars, 2 will be manual and 3 will be automatic

-   The *log-odds* of a car being a manual is $log(0.66)=-0.41$

**INTERPETING THE COEFICIENTS:**

-   The coefficient $\beta=0.31$ shows the expected change in the log-odds that the car is a manual, as MPG (the continuous predictor) increases by 1 point.

-   The odds ratio for the same coefficient shows that *for every 1 MPG increase in fuel efficiency, there is a 1.4 times increase in the odds* of of the car being an automatic. Note that the increase is describing a change in odds, ***NOT*** x% "more likely"/how likely the outcome is.

-   (if this was a categorical variable, it would say that the odds are x% higher for group A vs. group B)
