---
title: "Untitled"
output: html_document
date: "2023-04-25"
---

```{r setup}

pacman::p_load(tidyverse, brms, easystats, loo)
options(mc.cores = parallel::detectCores(logical = FALSE)-1)


# Load custom function
loo_compare_tidy=function(...){
  loo::loo_compare(..., criterion = c("loo")) |>
    data.frame() |> 
    rownames_to_column(var = "model") |> 
    as_tibble() |> 
    mutate(across(c(2:9), round, 2)) # round columns 2-9 to 2 decimals
}

# Read in practice models from McElreath ch. 7
b6.6=readRDS(file= here::here("fits", "b06.06.rds"))
b6.7=readRDS(file= here::here("fits", "b06.07.rds"))
b6.8=readRDS(file= here::here("fits", "b06.08.rds"))
```

# Part 3: Comparing Competing Models

There are different methods of model comparison that are possible in the Bayesian framework, and the version to use depends on your model's overall goals/purpose.

-   Comparing models based on their *ability to predict a new set of unobserved data*, and for guarding against over-fitting (via Cross-Validation and/or Information Criteria). CV and IC are two **distinct, yet very similar performing** ways to estimate model performance with new data.

-   **Bayes factors** can also be used to compare statistical models by answering the question: *Under which model are the the observed data more probable?* In other words, is an effect supported by the data, and *which model is more likely to have produced the observed data?* Thus, the Bayes factor is a measure of the relative evidence of one of the compared models over the other.

Note that "*many Bayesians*" do not like Bayes factors and warn against their use. In part, because "*even when priors are weak and have little influence on the posterior distributions within models, priors can have a huge impact on comparisons between models*." (McElreath, chapter 7). See also [Kruschke's very good criticisms and warnings of them](https://link.springer.com/article/10.3758/s13423-017-1272-1?wt_mc=Other.Other.8.CON1172.PSBR%20VSI%20Art01%20&%20utm_medium=other%20&%20utm_source=other%20&%20utm_content=2062018%20&%20utm_campaign=8_ago1936_psbr%20vsi%20art01) when used specifically within the context of comparing models that represent the Null and alternative hypotheses; and [Makowski et al.'s](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full) recommendation for when they are appropriate vs. when they are not.

Further, [Vehtari mentions here that](https://avehtari.github.io/modelselection/CV-FAQ.html#24_What_is_the_relationship_between_LOO-CV_and_Bayes_factor):

> A Bayes factor can be sensible when models are well specified and there is lot of data compared to the number of parameters, so that maximum a posteriori estimate is fine and the result is not sensitive to priors...If there is not a lot of data compared to the number of parameters, Bayes factor can be much more sensitive to prior choice than LOO-CV.

## Example from Statistical Rethinking

For newly created models, the first thing to do is compute and add loo and WAIC info to the model object.

```{r}

b6.6 <- add_criterion(b6.6, criterion = c("waic", "loo"))
b6.7 <- add_criterion(b6.7, criterion = c("waic", "loo"))
b6.8 <- add_criterion(b6.8, criterion = c("waic", "loo"))
```

<https://avehtari.github.io/modelselection/CV-FAQ.html#5_How_to_use_cross-validation_for_model_selection>

> ***HOW TO USE CROSS-VALIDATION FOR MODEL SELECTION?***
>
> First avoid model selection by using the model which includes all predictors and includes all uncertain things. Then optimal thing is to integrate over all the uncertainties. When including many components to a model, it is useful to think more carefully about the prior. For example, if there are many predictors, it is useful to use priors that a) state that only some of the effects are big, or b) many effects are big and correlating (it is not possible to have a large number of big independent effects Tosh *et al.* ([2021](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Tosh+etal:2021:piranha))).
>
> If there is explicit utility or loss for observing future predictor values (e.g. medical tests) use decision theory.
>
> If there is implicit cost for bigger models (e.g. bigger model more difficult to explain or costs of feature measurements are unknown), choose a smaller model which similar predictive performance as the biggest model. If there are only a small number of models, overfitting due to selection process is small. If there are a large number of models, as for example often in variable selection, then the overfitting due to the selection process can be a problem (Piironen and Vehtari, [2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Piironen+Vehtari:2017a)) and more elaborate approaches, such as projection predictive variable selection is recommended.
>
> If there is application specific utility or loss function, use that to assess practically relevant difference in predictive performance of two models.
>
> **If there is no application specific utility or loss function, use log score, ie elpd**.
>
> -   If elpd difference (elpd_diff in `loo` package) is less than 4, the difference is small (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty))).
>
> -   If elpd difference (elpd_diff in loo package) is larger than 4, then compare that difference to standard error of elpd_diff (provided e.g. by `loo` package) (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty)). See also Section [How to interpret in Standard error (SE) of elpd difference (elpd_diff)?](https://avehtari.github.io/modelselection/CV-FAQ.html#se_diff).
>
> If there is a large number of models compared, there is possibility of overfitting in model selection. See video [Model assessment, comparison and selection at Master class in Bayesian statistics, CIRM, Marseille](https://www.youtube.com/watch?v=Re-2yVd0Mqk).
>
> -   Vehtari and Ojanen ([2012](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+Ojanen:2012)) write: "The model selection induced bias can be taken into account by the double/nested/2-deep cross-validation (e.g. Stone, 1974; Jonathan, Krzanowski and McCarthy, 2000) or making an additional bias correction (Tibshirani and Tibshirani, 2009)."
>
> -   Piironen and Vehtari ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Piironen+Vehtari:2017a)) write: "Although LOO-CV and WAIC can be used to obtain a nearly unbiased estimate of the predictive ability of a given model, both of these estimates contain a stochastic error term whose variance can be substantial when the dataset is not very large. This variance in the estimate may lead to over-fitting in the selection process causing nonoptimal model selection and inducing bias in the performance estimate for the selected model (e.g., Ambroise and McLachlan 2002; Reunanen 2003; Cawley and Talbot 2010). The overfitting in the selection may be negligible if only a few models are being compared but, as we will demonstrate, may become a problem for a larger number of candidate models, such as in variable selection."
>
> -   Nested CV helps to estimate the overfitting due to the selection but doesn't remove that. The overfitting is more severe depending on how many degrees of freedom there are in the selection. For example, in predictor selection we can think that we as many indicator variables as there are predictors and then there are combinatorial explosion in possible parameter combinations and overfitting can be severe (as demonstrated by Piironen and Vehtari ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Piironen+Vehtari:2017a))).

This picture may also help explain ELPD

```{r}
knitr::include_graphics(here::here("images", "elpd.png"))
```

```{r}

loo_estimates=loo_compare_tidy(b6.6, b6.7, b6.8)
loo_estimates
```

### Expected Log-Predictive Density (ELPD) and LOO-CV

[Recommendation from Stan team](https://discourse.mc-stan.org/t/understanding-looic/13409/4) is to look at ELPD, not LOOIC, when examining model predictions/quality.

`elpd_loo` is a point estimate that represents the expected out-of-sample performance of a model, which is calculated using PSIS-LOO-CV (an approximation of leave-one-out cross-validation). **ELPD quantifies the average log-likelihood of new data points given the model's posterior distribution (i.e., it quantifies how well the model is able to predict new data).** It takes into account both the model's ability to fit the observed data (log-likelihood) and its complexity (penalization for model complexity).

-   The larger the ELPD, the better the model is at predicting new, unobserved data. (Note that these values only have meaning *relative to other ELPD's, however; you can't interpret them on their own.*)

However, the difference between models' ELPD is a difference score that requires a separate estimation from computing just the raw model ELPD (similar to how you need to compute contrasts in order to compare estimated means). So you want to compare models based on `elpd_diff`. `elpd_diff` shows the difference between models' ELPD; it's a direct comparison of their estimated out-of-sample prediction capabilities. `elpd_se` is the standard error associated with this estimate, which shows how reliable this estimate is.

"[As quick rule](https://avehtari.github.io/modelselection/CV-FAQ.html#12_What_is_the_interpretation_of_ELPD__elpd_loo__elpd_diff),"

> If elpd difference (`elpd_diff` in `loo` package) is less than 4, the difference is small [and the models have "very similar predictive performance"] (Sivula, Magnusson and Vehtari, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Sivula+etal:2020:loo_uncertainty)). If elpd difference is larger than 4, then compare that difference to standard error of `elpd_diff`...[if the size of the difference is greater than two SE's, than that could be meaningful.]

`p_loo` is the effective number of parameters in the model. It is a penalty term applied to each model that represents model complexity. Per Vehtari:

> It is not needed...but has diagnostic value. It describes how much more difficult it is to predict future data than the observed data. Asymptotically under certain regularity conditions, p_loo can be interpreted as the effective number of parameters.
>
> p_loo \>N or p_loo \>p indicates that the model has very weak predictive capability. This can happen even in case of well specified model (as demonstrated in Figure 1 in Vehtari, Gelman and Gabry (2017)), but may also indicate a severe model misspecification.

### Information Criteria

`looic` and `elpd_waic` are Info Criterion based the concept of cross-validation. [They both estimate ELPD](https://avehtari.github.io/modelselection/CV-FAQ.html#21_How_are_LOO_and_WAIC_related)***.*** The model is trained with either point i left out (LOOIC) or several points left out at once (WAIC); the log-likelihood of the left-out data is recorded; and the process repeated *k* times.

-   LOOIC is based on LOO, where WAIC is a different calculation/approach entirely.

-   WAIC and LOOIC should produce very similar results and be very close in their estimates, even though they are computed differently and handle model complexity differently. Contrary to ELPD above, *lower values indicate better performance.*

-   However, note the following from [Vehtari's blog](Vehtari,%20Gelman%20and%20Gabry%20(2017)%20show%20that%20PSIS-LOO%20has%20usually%20smaller%20error%20in%20estimating%20ELPD%20than%20WAIC.%20The%20exception%20is%20the%20case%20when%20p_loo%20≪N,%20as%20then%20WAIC%20tends%20to%20have%20slightly%20smaller%20error,%20but%20in%20that%20case%20both%20PSIS-LOO%20and%20WAIC%20have%20very%20small%20error%20and%20it%20doesn’t%20matter%20which%20computational%20approximation%20is%20used.%20On%20the%20other%20hand,%20for%20flexible%20models%20WAIC%20fails%20more%20easily,%20has%20significant%20bias%20and%20is%20less%20easy%20to%20diagnose%20for%20failures.%20WAIC%20has%20been%20included%20in%20loo%20package%20only%20for%20comparison%20purposes%20and%20to%20make%20it%20easy%20to%20replicate%20the%20results%20in%20Vehtari,%20Gelman%20and%20Gabry%20(2017).), that **suggests you should probably prefer LOOIC to WAIC:**

> **"**Vehtari, Gelman and Gabry ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017)) show that PSIS-LOO has usually smaller error in estimating ELPD than WAIC. The exception is the case when p_loo ≪N, as then WAIC tends to have slightly smaller error, but in that case both PSIS-LOO and WAIC have very small error and it doesn't matter which computational approximation is used. On the other hand, for flexible models WAIC fails more easily, has significant bias and is less easy to diagnose for failures. WAIC has been included in `loo` package only for comparison purposes and to make it easy to replicate the results in Vehtari, Gelman and Gabry ([2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017))

Comparing model b6.7 to b6.8:

```{r}
loo_estimates |> slice(1:2) |> select(1:3)
```

There is a large difference between their estimated ELPD, such that b6.7 is estimated to be much better at predicting new data than b6.8.

However, if we compare b6.6 to b6.8...

```{r}
loo_estimates |> slice(1,3) |> select(1:3)
```

That is a large difference between the models (`elpd_diff`\> 4), so now we check the SE estimate for that difference. [Per Vehtari](https://discourse.mc-stan.org/t/understanding-looic/13409/6), you want to see that $ELPD_{diff} > 4 * SE_{diff}$

**BUT REMEMBER:** Just because a model with an extra predictor is making better predictions, that does not imply that model is a better causal or theoretical model. It just means it has something in it that's associated with the outcome and "*if you select a model based only on expected predictive accuracy, you could easily be confounded."* Do not automatically discard a model out of hand because it has worse performance; consider the relationships between variables and different possible causal models, and the potential for confounds or spurious relationships that may affect the models.

Alsoconsider also that "*...in the natural and social sciences the models under consideration are almost never the data-generating models. It makes little sense to attempt to identify a 'true' model."* (McElreath, chapter 7).

**ALTERNATIVELY, Z's textbook also recommended this other approach**: Check to see that the difference between the models' ELPD is at least 2 standard error's below zero.

$$
-0.87_{diff}+(2*0.35_{se})=0.7
$$

$$
-0.87_{diff}+0.7=-0.17
$$

Since $-0.17<0$, there is a meaningful difference between the models.

## Pareto-K and Outliers

For this example, which shows how outliers can negatively affect posterior predictions, use the following McElreath models:

```{r}
b5.1=readRDS(file= here::here("fits", "b05.01.rds"))
b5.2=readRDS(file= here::here("fits", "b05.02.rds"))
b5.3=readRDS(file= here::here("fits", "b05.03.rds"))
```

[https://avehtari.github.io/modelselection/CV-FAQ.html#17_What_to_do_if_I_have_many_high_Pareto\_(hat{k})%E2%80%99s](https://avehtari.github.io/modelselection/CV-FAQ.html#17_What_to_do_if_I_have_many_high_Pareto_(hat%7Bk%7D)%E2%80%99s)

> The Pareto-k\^ diagnostic estimates how far an individual leave-one-out distribution is from the full distribution. If leaving out an observation changes the posterior too much then importance sampling is not able to give reliable estimate.
>
> -   If k\^\<0.5, then the corresponding component of `elpd_loo` is estimated with high accuracy.
>
> -   If 0.5\<k\^\<0.7 the accuracy is lower, but still OK.
>
> -   If k\^\>0.7, then importance sampling is not able to provide useful estimate for that component/observation.
>
> Pareto-k\^ is also useful as a measure of influence of an observation. Highly influential observations have high k\^ values. Very high k\^ values often indicate model misspecification, outliers or mistakes in data processing. In cases where k\>0.7, check `p_loo`...
>
> -   If p_loo ≪p (the total number of parameters in the model), then the model is likely to be misspecified. Posterior predictive checks (PPCs) are then likely to also detect the problem. Try using an overdispersed model, or add more structural information (nonlinearity, mixture model, etc.).
>
> -   If p_loo \<p and the number of parameters p is relatively large compared to the number of observations (e.g., p\>N/5), it is likely that the model is so flexible or the population prior so weak that it's difficult to predict the left out observation (even for the true model). This happens, for example, in the simulated 8 schools (Vehtari, Gelman and Gabry, [2017](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Vehtari+etal:PSIS-LOO:2017)), random effect models with a few observations per random effect, and Gaussian processes and spatial models with short correlation lengths.
>
> -   If p_loo \>p, then the model is likely to be badly misspecified. If the number of parameters p≪N, then PPCs are also likely to detect the problem. See for example the [Roaches case study](https://avehtari.github.io/modelselection/roaches.html). If p is relatively large compared to the number of observations, say p\>N/5 (more accurately we should count number of observations influencing each parameter as in hierarchical models some groups may have few observations and other groups many), it is possible that PPCs won't detect the problem.

```{r}
loo(b5.3)
```

Model 5.3 has one estimate in the "ok" range. So there's at least one piece of data that didn't quite measure up to "good" status. Which piece was that? Let's filter by using the minimum upper bound specified for "good."

```{r}
loo(b5.3) |> pareto_k_ids(threshold = 0.5)
```

Looks like it was the value in the 13th row of the data. You can filter your data to find out which point that is.

You can also check the specific pareto-k value for that individual data point.

```{r}
b5.3$criteria$loo$diagnostics$pareto_k[13]
```

In this case, that value isn't great, but not bad enough (i.e., at or worse than .7) to return a warning message.

### What to do when you have overly-influential observations?

One option is to use a more robust test, such as "robust regression" with a student-t distribution instead of a normal distribution.

## Bayes factors

A Bayes factor answers the question, ***under which model are the observed data more probable? Which model is more likely to have produced the data?***

[From Vehtari's blog](https://avehtari.github.io/modelselection/CV-FAQ.html#24_What_is_the_relationship_between_LOO-CV_and_Bayes_factor):

### How does a Bayes factor compare to LOO-CV?

> -   LOO-CV estimates the predictive performance given N−1 observations. Bayes factor can be presented as ratio of predictive performance estimates given 0 observations. Alternatively Bayes factor can be interpreted as choosing the maximum a posterior model.
>
> -   Bayes factor can be sensible when models are well specified and there is lot of data compared to the number of parameters, so that maximum a posteriori estimate is fine and the result is not sensitive to priors
>
> -   If there is not a lot of data compared to the number of parameters, Bayes factor can be much more sensitive to prior choice than LOO-CV
>
> -   If the models are not very close to the true model, Bayes factor can be more unstable than cross-validation (Yao *et al.*, [2018](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Yao+etal:2018); Oelrich *et al.*, [2020](https://avehtari.github.io/modelselection/CV-FAQ.html#ref-Oelrich+etal:2020:overconfident)).
>
> -   Computation of Bayes factor is more challenging. For example, if computed from MCMC sample, usually several orders of magnitude bigger sample sizes are needed for Bayes factor than for LOO-CV
>
> -   If the models are well specified, regular, and there is a lot of data compared to the number of parameters (n≫p), then Bayes factor may have smaller variance than LOO-CV. If the models are nested, instead of Bayes factor, it is also possible to look directly at the posterior of the interesting parameters (see also 2b in [Using cross-validation for many models](https://avehtari.github.io/modelselection/CV-FAQ.html#manymodels))

Note: The code below is copied from the `bayestestR` website. To view their tutorial in full, see: <https://easystats.github.io/bayestestR/articles/bayes_factors.html#bayesfactor_models>

```{r}

# intercept only model
m0 <- brm(Sepal.Length ~ 1,
  data = iris,
  prior =
    set_prior("student_t(3, 6, 6)", class = "Intercept") +
      set_prior("student_t(3, 0, 6)", class = "sigma"),
  chains = 10, iter = 5000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

# Petal.Length only
m1 <- brm(Sepal.Length ~ Petal.Length,
  data = iris,
  prior =
    set_prior("student_t(3, 6, 6)", class = "Intercept") +
      set_prior("student_t(3, 0, 6)", class = "sigma") +
      set_prior("normal(0, 1)", coef = "Petal.Length"),
  chains = 10, iter = 5000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

# Species only
m2 <- brm(Sepal.Length ~ Species,
  data = iris,
  prior =
    set_prior("student_t(3, 6, 6)", class = "Intercept") +
      set_prior("student_t(3, 0, 6)", class = "sigma") +
      set_prior("normal(0, 3)", coef = c("Speciesversicolor", "Speciesvirginica")),
  chains = 10, iter = 5000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

# Species + Petal.Length model
m3 <- brm(Sepal.Length ~ Species + Petal.Length,
  data = iris,
  prior =
    set_prior("student_t(3, 6, 6)", class = "Intercept") +
      set_prior("student_t(3, 0, 6)", class = "sigma") +
      set_prior("normal(0, 1)", coef = "Petal.Length") +
      set_prior("normal(0, 3)", coef = c("Speciesversicolor", "Speciesvirginica")),
  chains = 10, iter = 5000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

# full interactive model
m4 <- brm(Sepal.Length ~ Species * Petal.Length,
  data = iris,
  prior =
    set_prior("student_t(3, 6, 6)", class = "Intercept") +
      set_prior("student_t(3, 0, 6)", class = "sigma") +
      set_prior("normal(0, 1)", coef = "Petal.Length") +
      set_prior("normal(0, 3)", coef = c("Speciesversicolor", "Speciesvirginica")) +
      set_prior("normal(0, 2)", coef = c("Speciesversicolor:Petal.Length", "Speciesvirginica:Petal.Length")),
  chains = 10, iter = 5000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)


#### Compute Bayes factors ####

comparison <- bayesfactor_models(m1, m2, m3, m4, denominator = m0)
comparison

# have easystats do the interpretation for you
comparison |> 
  as_tibble() |> 
  mutate(interpretation=effectsize::interpret_bf(log_BF))

# can also use my custom function
legaldmlab::tidy_bf_models(comparison)


#### Create a publication-ready table ####

BF_Models_table=legaldmlab::APA_table(BF_Models_table, 
                                      include_note = "Bayes factors were interpreted according to Jeffrys' criteria")

#### Save output ####
save_table(flextable_object = BF_Models_table, 
           file_path = here::here("Figures and Tables"),
           file_name = "Model_Comparison.docx")
```
