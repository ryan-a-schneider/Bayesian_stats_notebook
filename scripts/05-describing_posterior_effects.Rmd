```{r setup, include=FALSE}

pacman::p_load(brms, tidyverse, easystats)
options(mc.cores = parallel::detectCores(logical = FALSE)-1)
```

# Part 4: Examining the Posterior

[Makowski et al. (2019)](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full) recommends the following:

-   At least one index of effect *existence* (preferably the **Probability of direction**)
-   At least one index of effect *significance* (either the **ROPE test or a Bayes factor**, depending on the situation)
    -   If you have little prior information to go off of in your study and thus have used *weak priors* and are *unsure what to expect* regarding the effect size, [you should report the ROPE test]{.underline}*.*

    -   If you used more informative priors and have a more clear idea of the expected effect size, [use Bayes factors.]{.underline} The following commands are your best options for examining posterior effects:

```{r}
brms::fixef(b5.3) # uses quantile-based intervals only

broom.mixed::tidyMCMC(b5.3, conf.method = "HPDinterval", conf.int = TRUE, conf.level = .95) # can change intervals

bayestestR::describe_posterior(b5.3, ci_method = "HDI", ci = .95, rope_range = c(-.25, .25)) # most detailed
```

-   Do note though that the table that is printed in the console/in RMarkdown when you use `describe_posterior` is cleaned up and formatted...if you save this table in the environment, it will be a regular data frame with un-rounded columns, etc. If you want to save the table exactly how it is printed in the console (i.e., rounded to two decimals, ROPE and CI columns combined in APA style, etc.), to export an APA formatted table, pipe `describe_posterior` to `insight::format_table()`.

## Effect Existence

The most basic inferential statistic, it is the maximal probability the our estimate is strictly directional; that is, larger or smaller than 0, and it generally ranges from 50% (no preference) to 100%.

Moreover, it is strongly correlated with the frequentest p-value, and can thus be used to draw parallels and give some reference to readers non-familiar with Bayesian statistics. A two-sided p-value of respectively .1, .05, .01 and .001 would correspond approximately to a pd of 95%, 97.5%, 99.5% and 99.95%. Thus, for convenience, we suggest the following reference values as an interpretation helpers:

```         
pd <= 95% ~ p > .1: uncertain
pd > 95% ~ p < .1: possibly existing
pd > 97%: likely existing
pd > 99%: probably existing
pd > 99.9%: certainly existing
```

Note that a low *pd* is similar to a p-value in that it does not tell you an effect does not exist; rather, it says you have uncertainty about whether it exists.

## Effect Magnitude (via ROPE)

`rope()` computes the portion of the HDI (defualt to 89% of HDI) of a posterior distribution that lies within a Region of Practical Equivilence (ROPE).

*This is used instead of a Null Hypothesis test because statistically, testing that the probability of a posterior distribution being different from 0 does not make sense (i.e., the probability of being different from any single point is infinite). The purpose therefore of using a ROPE test is to let the user define an area around the null value enclosing values that would be equivalent to the null, for practical purposes.*

You can do a ROPE test two different ways:

1.  Test the full posterior to see what percentage of *all possible values* fall in the null band (this is called the full ROPE), or

2.  Test against the most probable values (those within the 89% or 95% HDI).

Either method is fine (although the full ROPE is more sensitive to delineating highly-significant effects), and the results are interpreted similarly: The proportion of the HDI inside this "null" region can be used as a decision criterion for "NH" testing; that is, a test for "Practical Equivalence", or that an effect is large enough to have "practical" (as opposed to purely statistical) significance.

Also in either case, you are encouraged to interpret the results of the test in a continuous and probabilistic (i.e., not black-and-white or all-or-nothing) sense; do not use it like a Null Hypothesis test. ROPE is an index that tells you whether a parameter is related - or not - to a non-negligible change (in terms of magnitude) in the outcome. However, based on simulation data, we suggest the following reference values as an interpretation helpers:

```         
> 99% in ROPE: negligible (we can accept the null hypothesis)

> 97.5% in ROPE: probably negligible

<= 97.5% & >= 2.5% in ROPE: undecided significance

< 2.5% in ROPE: probably significant

< 1% in ROPE: significant (we can reject the null hypothesis)
```

*Note that extra caution is required as its interpretation highly depends on other parameters such as sample size and ROPE range*. See [this link](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html#sensitivity-to-parameters-scale).

### Notes on using the ROPE from my meeting with Mike

-   The default recommended heuristic for a ROPE size is [-0.1, 0.1] for a standardized parameter. You can use `bayestestR::rope_range()` to find out what that would be for your variable of interest.

-   However, choosing a ROPE boundary should be done with caution, and generally not left up to a heuristic. It should define the line between "so small as to be meaningless" and "small, but large enough to be meaningful or useful."

-   In my dissertation, my originally chosen ROPE of [-0.2, 0.2] was the size of what's conventionally considered a small effect (Cohen's d). That's not necessarily meaningless...especially when you consider that was a third of my anticipated effect size (Cohen's d = 0.6).

## FUN SURPRISE

Instead of interpreting the values in `describe_posterior` or some equivalent yourself, you can have easystats do it for you!

```{r}
bayestestR::sexit(b5.3)

report::report_model(b5.3)
```
